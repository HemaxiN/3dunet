{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from keras import backend as K\n",
    "from tensorflow.python.ops import *\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.engine import Input, Model\n",
    "from keras.layers import Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU\n",
    "from keras.optimizers import Adam\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "try:\n",
    "        from keras.engine import merge\n",
    "except ImportError:\n",
    "        from keras.layers.merge import concatenate\n",
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras \n",
    "import keras.backend as K\n",
    "from keras.losses import categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, Lambda, multiply\n",
    "from keras import layers as L\n",
    "from tensorflow.python.ops import *    \n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tensorflow.python.ops import *\n",
    "\n",
    "def mse(y_true, y_pred, sample_weight=None):\n",
    "    #y_pred = tf.convert_to_tensor_v2(y_pred)\n",
    "    #y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    squared  = math_ops.square(y_pred - y_true)\n",
    "    if sample_weight==None:\n",
    "        return tf.reduce_mean(squared)\n",
    "    else:\n",
    "        multiplication = math_ops.multiply(sample_weight, squared)\n",
    "        return tf.reduce_mean(multiplication)\n",
    "\n",
    "def mean_se(y_true, y_pred):\n",
    "    [weight1, vecxgt, vecygt, veczgt] = tf.unstack(y_true, 4, axis=4)\n",
    "    [vecx, vecy, vecz] = tf.unstack(y_pred, 3, axis=4)\n",
    "    vecx = tf.expand_dims(vecx, -1)\n",
    "    vecxgt = tf.expand_dims(vecxgt, -1)\n",
    "    vecy = tf.expand_dims(vecy, -1)\n",
    "    vecygt = tf.expand_dims(vecygt, -1)\n",
    "    vecz = tf.expand_dims(vecz, -1)\n",
    "    veczgt = tf.expand_dims(veczgt, -1)\n",
    "    vecx = K.flatten(vecx)\n",
    "    vecxgt = K.flatten(vecxgt)\n",
    "    vecy = K.flatten(vecy)\n",
    "    vecygt = K.flatten(vecygt)\n",
    "    vecz = K.flatten(vecz)\n",
    "    veczgt = K.flatten(veczgt)\n",
    "    epe_loss_channelx = epe_loss(vecx, vecxgt)\n",
    "    epe_loss_channely = epe_loss(vecy, vecygt)\n",
    "    epe_loss_channelz = epe_loss(vecz, veczgt)\n",
    "    return 0.33*epe_loss_channelx + 0.33*epe_loss_channely + 0.33*epe_loss_channelz\n",
    "\n",
    "def epe_loss(y_true, y_pred):\n",
    "        output = mse(y_true, y_pred, sample_weight=None)\n",
    "        return output\n",
    "\n",
    "def epe_loss1(y_true, y_pred, weight):\n",
    "        output = mse(y_true, y_pred, sample_weight = weight)\n",
    "        return output\n",
    "\n",
    "def wbce(seg, y_pred):\n",
    "    epsilon = tf.convert_to_tensor(10e-8, y_pred.dtype.base_dtype)\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "    first_term = math_ops.multiply(seg, tf.log(y_pred))\n",
    "    second_term = math_ops.multiply(1-seg, tf.log(1-y_pred))\n",
    "    entropy = first_term + second_term\n",
    "    entropy = -entropy\n",
    "    multiplication = entropy\n",
    "    return tf.reduce_mean(multiplication)\n",
    "\n",
    "def dc0(y_true, y_pred):\n",
    "    smooth=1e-3\n",
    "    [first_gt, second_gt, weight1, weight2, vecx, vecy, vecz] = tf.unstack(y_true, 7, axis=4)\n",
    "    [first_pred, second_pred, vecxgt, vecygt, veczgt] = tf.unstack(y_pred, 5, axis=4)\n",
    "    y_true_f = K.flatten(first_gt)\n",
    "    y_pred_f = K.flatten(first_pred)\n",
    "    intersection = K.sum(y_true_f*y_pred_f)\n",
    "    return K.mean((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "\n",
    "\n",
    "def dc1(y_true, y_pred):\n",
    "    smooth=1e-3\n",
    "    [first_gt, second_gt, weight1, weight2, vecxgt, vecygt, veczgt] = tf.unstack(y_true, 7, axis=4)\n",
    "    [first_pred, second_pred, vecx, vecy, vecz] = tf.unstack(y_pred, 5, axis=4)\n",
    "    y_true_f = K.flatten(second_gt)\n",
    "    y_pred_f = K.flatten(second_pred)\n",
    "    intersection = K.sum(y_true_f*y_pred_f)\n",
    "    return K.mean((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "\n",
    "\n",
    "def weighted_joint_loss_function(y_true, y_pred):\n",
    "    [weight1, vecx, vecy, vecz] = tf.unstack(y_true, 4, axis=4)\n",
    "    [vecxgt, vecygt, veczgt] = tf.unstack(y_pred, 3, axis=4)\n",
    "    weight1 = tf.expand_dims(weight1, -1)\n",
    "    vecx = tf.expand_dims(vecx, -1)\n",
    "    vecy = tf.expand_dims(vecy, -1)\n",
    "    vecz = tf.expand_dims(vecz, -1)\n",
    "    vecxgt = tf.expand_dims(vecxgt, -1)\n",
    "    vecygt = tf.expand_dims(vecygt, -1)\n",
    "    veczgt = tf.expand_dims(veczgt, -1)\n",
    "    mse_vectorsx = epe_loss1(vecx, vecxgt, weight1)\n",
    "    mse_vectorsy = epe_loss1(vecy, vecygt, weight1)\n",
    "    mse_vectorsz = epe_loss1(vecz, veczgt, weight1)\n",
    "    return 0.33*mse_vectorsx + 0.33*mse_vectorsy + 0.33*mse_vectorsz + (1e-11)*(K.sum(K.abs(vecxgt))+K.sum(K.abs(vecygt))+K.sum(K.abs(veczgt)))\n",
    "\n",
    "\n",
    "def unet_model(n_classes=5, im_sz=128, depth=64, n_channels=2, n_filters_start=8, growth_factor=2, upconv=True):\n",
    "        droprate=0.05\n",
    "        n_filters = n_filters_start\n",
    "        inputs = Input((im_sz, im_sz, depth, n_channels))\n",
    "        #inputs = BatchNormalization(axis=-1)(inputs)\n",
    "        conv1 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(inputs)\n",
    "        conv1 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv1)\n",
    "        pool1 = MaxPooling3D(pool_size=(2, 2, 2), data_format='channels_last')(conv1)\n",
    "        #pool1 = Dropout(droprate)(pool1)\n",
    "        n_filters *= growth_factor\n",
    "        pool1 = BatchNormalization(axis=-1)(pool1)\n",
    "        conv2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(pool1)\n",
    "        conv2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv2)\n",
    "        pool2 = MaxPooling3D(pool_size=(2, 2, 2), data_format='channels_last')(conv2)\n",
    "        pool2 = Dropout(droprate)(pool2)\n",
    "        n_filters *= growth_factor\n",
    "        pool2 = BatchNormalization(axis=-1)(pool2)\n",
    "        conv3 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(pool2)\n",
    "        conv3 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv3)\n",
    "        pool3 = MaxPooling3D(pool_size=(2, 2, 2), data_format='channels_last')(conv3)\n",
    "        pool3 = Dropout(droprate)(pool3)\n",
    "        n_filters *= growth_factor\n",
    "        pool3 = BatchNormalization(axis=-1)(pool3)\n",
    "        conv4_0 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(pool3)\n",
    "        conv4_0 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv4_0)\n",
    "        pool4_1 = MaxPooling3D(pool_size=(2, 2, 2), data_format='channels_last')(conv4_0)\n",
    "        pool4_1 = Dropout(droprate)(pool4_1)\n",
    "        n_filters *= growth_factor\n",
    "        pool4_1 = BatchNormalization(axis=-1)(pool4_1)\n",
    "        conv4_1 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(pool4_1)\n",
    "        conv4_1 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv4_1)\n",
    "        pool4_2 = MaxPooling3D(pool_size=(2, 2, 2), data_format='channels_last')(conv4_1)\n",
    "        pool4_2 = Dropout(droprate)(pool4_2)\n",
    "        n_filters *= growth_factor\n",
    "        conv5 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(pool4_2)\n",
    "        conv5 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv5)\n",
    "        \n",
    "        n_filters //= growth_factor\n",
    "        if upconv:\n",
    "                up6_1 = concatenate([Conv3DTranspose(n_filters, (2, 2, 2), strides=(2, 2, 2), padding='same', data_format='channels_last')(conv5), conv4_1])\n",
    "        else:\n",
    "                up6_1 = concatenate([UpSampling3D(size=(2, 2, 2))(conv5), conv4_1])\n",
    "        up6_1 = BatchNormalization(axis=-1)(up6_1)\n",
    "        conv6_1 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(up6_1)\n",
    "        conv6_1 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv6_1)\n",
    "        conv6_1 = Dropout(droprate)(conv6_1)\n",
    "\n",
    "        n_filters //= growth_factor\n",
    "        if upconv:\n",
    "                up6_2 = concatenate([Conv3DTranspose(n_filters, (2, 2, 2), strides=(2, 2, 2), padding='same', data_format='channels_last')(conv6_1), conv4_0])\n",
    "        else:\n",
    "                up6_2 = concatenate([UpSampling3D(size=(2, 2, 2))(conv6_1), conv4_0])\n",
    "        up6_2 = BatchNormalization(axis=-1)(up6_2)\n",
    "        conv6_2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(up6_2)\n",
    "        conv6_2 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv6_2)\n",
    "        conv6_2 = Dropout(droprate)(conv6_2)\n",
    "\n",
    "        n_filters //= growth_factor\n",
    "        if upconv:\n",
    "                up7 = concatenate([Conv3DTranspose(n_filters, (2, 2, 2), strides=(2, 2, 2), padding='same', data_format='channels_last')(conv6_2), conv3])\n",
    "        else:\n",
    "                up7 = concatenate([UpSampling3D(size=(2, 2, 2))(conv6_2), conv3])\n",
    "        up7 = BatchNormalization(axis=-1)(up7)\n",
    "        conv7 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(up7)\n",
    "        conv7 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv7)\n",
    "        conv7 = Dropout(droprate)(conv7)\n",
    "\n",
    "        n_filters //= growth_factor\n",
    "        if upconv:\n",
    "                up8 = concatenate([Conv3DTranspose(n_filters, (2, 2, 2), strides=(2, 2, 2), padding='same', data_format='channels_last')(conv7), conv2])\n",
    "        else:\n",
    "                up8 = concatenate([UpSampling3D(size=(2, 2, 2))(conv7), conv2])\n",
    "        up8 = BatchNormalization(axis=-1)(up8)\n",
    "        conv8 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(up8)\n",
    "        conv8 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv8)\n",
    "        conv8 = Dropout(droprate)(conv8)\n",
    "\n",
    "        n_filters //= growth_factor\n",
    "        if upconv:\n",
    "                up9 = concatenate([Conv3DTranspose(n_filters, (2, 2, 2), strides=(2, 2, 2), padding='same', data_format='channels_last')(conv8), conv1])\n",
    "        else:\n",
    "                up9 = concatenate([UpSampling3D(size=(2, 2, 2))(conv8), conv1])\n",
    "        conv9 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(up9)\n",
    "        conv9 = Conv3D(n_filters, (3, 3, 3), activation='relu', padding='same', data_format='channels_last')(conv9)\n",
    "\n",
    "        #conv10 = Conv3D(2, (1, 1, 1), activation='sigmoid', data_format='channels_last')(conv9)\n",
    "        conv11 = Conv3D(3, (1, 1, 1), activation='linear', data_format='channels_last')(conv9)\n",
    "\n",
    "        #model = Model(inputs=inputs, outputs=concatenate([conv10, conv11], axis=4))\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=conv11)    \n",
    "        model.compile(optimizer=Adam(), loss=weighted_joint_loss_function, metrics = [mean_se])\n",
    "        return model\n",
    "\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch, initial_lrate, drop, epochs_drop):\n",
    "        return initial_lrate * math.pow(drop, math.floor((1+epoch)/float(epochs_drop)))\n",
    "\n",
    "\n",
    "def get_callbacks(model_file, initial_learning_rate=0.0001, learning_rate_drop=0.5, learning_rate_epochs=None,\n",
    "                                    learning_rate_patience=50, logging_file=\"/dev/shm/3dvectors/logs_colab/training_vectors_ctr_71th.log\", verbosity=1,\n",
    "                                    early_stopping_patience=None):\n",
    "        callbacks = list()\n",
    "        callbacks.append(ModelCheckpoint(model_file, save_best_only=True))\n",
    "        callbacks.append(CSVLogger(logging_file, append=True))\n",
    "        if learning_rate_epochs:\n",
    "                callbacks.append(LearningRateScheduler(partial(step_decay, initial_lrate=initial_learning_rate,\n",
    "                                                                                                             drop=learning_rate_drop, epochs_drop=learning_rate_epochs)))\n",
    "        else:\n",
    "                callbacks.append(ReduceLROnPlateau(factor=learning_rate_drop, patience=learning_rate_patience,\n",
    "                                                                                     verbose=verbosity))\n",
    "        if early_stopping_patience:\n",
    "                callbacks.append(EarlyStopping(verbose=verbosity, patience=early_stopping_patience))\n",
    "        return callbacks\n",
    "\n",
    "\n",
    "def load_old_model(model_file):\n",
    "        print(\"Loading pre-trained model\")\n",
    "        custom_objects = {'mean_se': mean_se, 'mse':mse, 'epe_loss':epe_loss, 'epe_loss1':epe_loss1,\n",
    "'weighted_joint_loss_function':weighted_joint_loss_function, 'dc0': dc0, 'dc1': dc1}\n",
    "        try:\n",
    "                from keras_contrib.layers import InstanceNormalization\n",
    "                custom_objects[\"InstanceNormalization\"] = InstanceNormalization\n",
    "        except ImportError:\n",
    "                pass\n",
    "        try:\n",
    "                return load_model(model_file,custom_objects=custom_objects)\n",
    "        except ValueError as error:\n",
    "                if 'InstanceNormalization' in str(error):\n",
    "                        raise ValueError(str(error) + \"\\n\\nPlease install keras-contrib to use InstanceNormalization:\\n\"\n",
    "                                                                                    \"'pip install git+https://www.github.com/keras-team/keras-contrib.git'\")\n",
    "                else:\n",
    "                        raise error\n",
    "\n",
    "\n",
    "def train_model(model, model_file, training_generator, validation_generator, steps_per_epoch, validation_steps,\n",
    "                                initial_learning_rate=0.001, learning_rate_drop=0.5, learning_rate_epochs=None, n_epochs=500,\n",
    "                                learning_rate_patience=20, early_stopping_patience=None):\n",
    "        \"\"\"\n",
    "        Train a Keras model.\n",
    "        :param early_stopping_patience: If set, training will end early if the validation loss does not improve after the\n",
    "        specified number of epochs.\n",
    "        :param learning_rate_patience: If learning_rate_epochs is not set, the learning rate will decrease if the validation\n",
    "        loss does not improve after the specified number of epochs. (default is 20)\n",
    "        :param model: Keras model that will be trained.\n",
    "        :param model_file: Where to save the Keras model.\n",
    "        :param training_generator: Generator that iterates through the training data.\n",
    "        :param validation_generator: Generator that iterates through the validation data.\n",
    "        :param steps_per_epoch: Number of batches that the training generator will provide during a given epoch.\n",
    "        :param validation_steps: Number of batches that the validation generator will provide during a given epoch.\n",
    "        :param initial_learning_rate: Learning rate at the beginning of training.\n",
    "        :param learning_rate_drop: How much at which to the learning rate will decay.\n",
    "        :param learning_rate_epochs: Number of epochs after which the learning rate will drop.\n",
    "        :param n_epochs: Total number of epochs to train the model.\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        model.fit_generator(generator=training_generator,\n",
    "                                                steps_per_epoch=steps_per_epoch,\n",
    "                                                epochs=n_epochs,\n",
    "                                                validation_data=validation_generator,\n",
    "                                                validation_steps=validation_steps,\n",
    "                                                callbacks=get_callbacks(model_file,\n",
    "                                                                                                initial_learning_rate=initial_learning_rate,\n",
    "                                                                                                learning_rate_drop=learning_rate_drop,\n",
    "                                                                                                learning_rate_epochs=learning_rate_epochs,\n",
    "                                                                                                learning_rate_patience=learning_rate_patience,\n",
    "                                                                                                early_stopping_patience=early_stopping_patience))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "import os\n",
    "from batchgenerators.dataloading.data_loader import SlimDataLoaderBase\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "from batchgenerators.augmentations.spatial_transformations import *\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "        def __init__(self, partition, configs, data_aug_dict=None):\n",
    "\n",
    "                self.data_aug_dict = data_aug_dict\n",
    "                self.partition = partition\n",
    "                self.list_IDs = sorted(os.listdir('/dev/shm/3dvectors/'+partition+'/images/'),key=self.order_dirs)\n",
    "\n",
    "                self.dim = configs['dim']\n",
    "                self.mask_dim = configs['mask_dim']\n",
    "                self.batch_size = configs['batch_size']\n",
    "                self.shuffle = configs['shuffle']\n",
    "                self.on_epoch_end()\n",
    "\n",
    "        def __len__(self):\n",
    "                'Denotes the number of batches per epoch'\n",
    "                return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "                'Generate one batch of data'\n",
    "                # Generate indexes of the batch\n",
    "                indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "                # Find list of IDs\n",
    "                list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "                # Generate data\n",
    "                X, mask = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "                if True:\n",
    "                        X, mask = self.augment(X,mask)\n",
    "                        X, mask = self.compute_weights(X,mask)\n",
    "                        #print('hey')\n",
    "\n",
    "                        #print(np.max(X))\n",
    "                        #print(np.shape(mask))  # (batch size, 256, 256, 64, 5)\n",
    "    \n",
    "                return X, mask\n",
    "\n",
    "        def on_epoch_end(self):\n",
    "                'Updates indexes after each epoch'\n",
    "                self.indexes = np.arange(len(self.list_IDs))\n",
    "                if self.shuffle == True:\n",
    "                        np.random.shuffle(self.indexes)\n",
    "\n",
    "        def __data_generation(self, list_IDs_temp):\n",
    "                # Initialization\n",
    "                X = np.empty((self.batch_size, *self.dim))\n",
    "                mask = []\n",
    "\n",
    "                # Generate data\n",
    "                for i, ID_path in enumerate(list_IDs_temp):\n",
    "\n",
    "                        X[i,] = np.load('/dev/shm/3dvectors/' + self.partition +'/images/'+ ID_path)\n",
    "\n",
    "                        mask.append(np.load('/dev/shm/3dvectors/' + self.partition +'/outputs/'+ ID_path))\n",
    "\n",
    "                return X, mask\n",
    "\n",
    "        def order_dirs(self, element):\n",
    "                return element.replace('.npy','')\n",
    "\n",
    "\n",
    "        def augment(self, X, mask):\n",
    "\n",
    "                X = self.rescale_img_values(X)\n",
    "\n",
    "                return X, mask\n",
    "\n",
    "        def rescale_img_values(self,img, max=None, min=None):\n",
    "\n",
    "                img = img/255.0\n",
    "\n",
    "                return img\n",
    "\n",
    "\n",
    "        def compute_weights(self, X, mask_gt):\n",
    "                #mask: list with N length, each n contains vectors for image n\n",
    "                IMG_SIZE = self.mask_dim[0]\n",
    "                _depth = self.mask_dim[2]\n",
    "                final_msk = np.empty((self.batch_size, *self.mask_dim))\n",
    "                iii = 0\n",
    "                for vec_array in mask_gt:\n",
    "\n",
    "                            img_aux = np.zeros((IMG_SIZE,IMG_SIZE,_depth,3))\n",
    "\n",
    "                            for v in vec_array:\n",
    "                                    if(int(v[1])<IMG_SIZE and int(v[0])<IMG_SIZE and int(v[2])<_depth):       \n",
    "                                    #if(True):   \n",
    "                                            img_aux[int(v[1]), int(v[0]), int(v[2]), 0] = v[3]\n",
    "                                            img_aux[int(v[1]), int(v[0]), int(v[2]), 1] = v[4]\n",
    "                                            img_aux[int(v[1]), int(v[0]), int(v[2]), 2] = v[5]\n",
    "\n",
    "                            masks_train = img_aux\n",
    "\n",
    "                            masks_train_a = np.zeros((np.shape(masks_train)[0], np.shape(masks_train)[1], _depth, np.shape(masks_train)[3]))\n",
    "                            masks_train_a[:,:,:,:] = masks_train[:,:,:,:] \n",
    "                            masks_train = masks_train_a\n",
    "\n",
    "                            masks = masks_train\n",
    "\n",
    "                            mask = masks[:,:,:,:]\n",
    "                            \n",
    "                            centroids_n = np.zeros(np.shape(mask[:,:,:,0]))\n",
    "                            \n",
    "                            nuclei_centroids_pred = []\n",
    "                            \n",
    "                            vectors_pred = masks[:,:,:,:] \n",
    "                            aux0 = np.array(np.where(vectors_pred[:,:,:,0]!=0)).T\n",
    "                            aux1 = np.array(np.where(vectors_pred[:,:,:,1]!=0)).T\n",
    "                            aux2 = np.array(np.where(vectors_pred[:,:,:,2]!=0)).T\n",
    "                            max_pos = np.argmax(np.asarray([len(aux0), len(aux1), len(aux2)]))\n",
    "                            if max_pos == 0:\n",
    "                                    a = aux0 \n",
    "                            elif max_pos == 1:\n",
    "                                    a = aux1\n",
    "                            else:\n",
    "                                    a = aux2\n",
    "                \n",
    "                            for v in a:\n",
    "                                    vx = vectors_pred[v[0],v[1],v[2], 0] \n",
    "                                    vy = vectors_pred[v[0],v[1],v[2], 1] \n",
    "                                    vz = vectors_pred[v[0],v[1],v[2], 2] \n",
    "                                    nuclei_centroids_pred.append([v[0], v[1], v[2]])\n",
    "\n",
    "                            for centroid in nuclei_centroids_pred:\n",
    "                                    if int(centroid[0])<IMG_SIZE and int(centroid[1])<IMG_SIZE:\n",
    "                                            centroids_n[int(centroid[0]), int(centroid[1]), int(centroid[2])] = 1\n",
    "                                    else:\n",
    "                                            centroids_n[int(centroid[0])-4, int(centroid[1])-4, int(centroid[2])] = 1\n",
    "                                            #maiu = 1\n",
    "\n",
    "                            centroids_ng = np.zeros((np.shape(centroids_n)[0],np.shape(centroids_n)[1], np.shape(centroids_n)[2],4))\n",
    "                            masks_aux_aux = 5*np.ones(np.shape(centroids_n))\n",
    "                            masks_aux_aux[centroids_n!=0] = 10000\n",
    "                            centroids_ng[:,:,:,0] = masks_aux_aux\n",
    "\n",
    "                            centroids_ng[:,:,:,1] = img_aux[:,:,:,0]\n",
    "                            centroids_ng[:,:,:,2] = img_aux[:,:,:,1]\n",
    "                            centroids_ng[:,:,:,3] = img_aux[:,:,:,2]\n",
    "\n",
    "                            final_msk[iii,:,:,:,:] = centroids_ng\n",
    "                            #np.save(os.path.join('/dev/shm/3dvectors/logs_colab', 'msk' + str(iii) + '.npy'), centroids_ng)\n",
    "                            #np.save(os.path.join('/dev/shm/3dvectors/logs_colab', 'img' + str(iii) + '.npy'), X[iii,])\n",
    "\n",
    "\n",
    "                            iii = iii + 1\n",
    "                return X, final_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "_size = 128\n",
    "_z_size = 64\n",
    "# Parameters\n",
    "data_train_configs = {'dim': (_size,_size,_z_size,2),\n",
    "                                        'mask_dim':(_size,_size,_z_size,4),\n",
    "                                        'batch_size': 16,\n",
    "                                        'shuffle': True}\n",
    "\n",
    "data_val_test_configs = {'dim': (_size,_size,_z_size,2),\n",
    "                                                'mask_dim':(_size,_size,_z_size,4),\n",
    "                                                'batch_size': 8,\n",
    "                                                'shuffle': True}\n",
    "\n",
    "training_configs = {\n",
    "                'model_file':'/mnt/2TBData/hemaxi/3dvectors/working_colab/vectors_ctr_71th.hdf5',\n",
    "                'initial_learning_rate':0.001,\n",
    "                'learning_rate_drop':0.8,\n",
    "                'learning_rate_patience':50,\n",
    "                'learning_rate_epochs':50, #ATENÃ‡ÃƒO\n",
    "                'early_stopping_patience':None,\n",
    "                'n_epochs':100,\n",
    "                }\n",
    "\n",
    "# Generators\n",
    "train_generator = DataGenerator(partition='train', configs=data_train_configs, data_aug_dict=None) \n",
    "validation_generator = DataGenerator(partition='val', configs=data_val_test_configs, data_aug_dict=None)\n",
    "test_generator = DataGenerator(partition='val', configs=data_val_test_configs, data_aug_dict=None)\n",
    "\n",
    "model = load_old_model('/mnt/2TBData/hemaxi/3dvectors/working_colab/final_vectors_ctr_70th.hdf5')\n",
    "\n",
    "train_model(model=model, training_generator=train_generator,\n",
    "                        validation_generator=validation_generator,\n",
    "                        steps_per_epoch=train_generator.__len__(),\n",
    "                        validation_steps=validation_generator.__len__(), **training_configs)\n",
    "\n",
    "model.save('/mnt/2TBData/hemaxi/3dvectors/working_colab/final_vectors_ctr_71th.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
